{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('.venv')"
  },
  "interpreter": {
   "hash": "9c40931cbb4a31228df7bc840fc8745c5509fc7c9b0d60dae36a005e5fe0dba9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Crystal graph convolution"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, ReLU, GELU, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import CGConv, global_add_pool, global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary"
   ]
  },
  {
   "source": [
    "## Load data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = '../data'\n",
    "PICKLE_FOLDER = '../pickles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PICKLE_FOLDER}/timeseries.pickle', 'rb') as f:\n",
    "    ts = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples, total_brain_regions, ts_length = ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata = pd.read_csv(f'{DATA_FOLDER}/patients-cleaned.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metadata.head(2)"
   ]
  },
  {
   "source": [
    "### Select connectivity dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "THRESHOLD = 0.1                                          # 0.01, 0.05, 0.1, 0.15\n",
    "N = 20 #                                                 # 3, 5, 7, 10, 15, 20, 40\n",
    "CORR_TYPE = 'pearson'                                    # 'pearson', 'spearman', 'partial-pearson'\n",
    "THRESHOLD_METHOD = 'abs-group-avg-diff'                  # 'abs-sample-diff', 'abs-group-avg-diff'\n",
    "THRESHOLD_TYPE = 'min'                                 # 'min', 'max' or for kNN 'small', 'large'\n",
    "KNN = False                                               # Whether all or only top N neigbors are taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_folder = f'{PICKLE_FOLDER}/fc-{CORR_TYPE}{\"-knn\" if KNN else \"\"}-{THRESHOLD_METHOD}'\n",
    "\n",
    "# Try Gini or SGD.\n",
    "# fc_folder = f'{PICKLE_FOLDER}/fc-{CORR_TYPE}-gini'\n",
    "# fc_folder = f'{PICKLE_FOLDER}/fc-{CORR_TYPE}-sgd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_file_binary = f'{fc_folder}/{THRESHOLD_TYPE}-{f\"knn-{N}\" if KNN else f\"th-{THRESHOLD}\"}-binary.pickle'\n",
    "fc_file_real = f'{fc_folder}/{THRESHOLD_TYPE}-{f\"knn-{N}\" if KNN else f\"th-{THRESHOLD}\"}-real.pickle'\n",
    "\n",
    "# fc_file_binary = f'{fc_folder}/binary.pickle'\n",
    "# fc_file_real = f'{fc_folder}/real.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fc_file_binary, 'rb') as f:\n",
    "    edge_index_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(fc_file_real, 'rb') as f:\n",
    "    fc_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_matrix.shape"
   ]
  },
  {
   "source": [
    "## Split data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PICKLE_FOLDER}/test-indices.pickle', 'rb') as f:\n",
    "    test_indices = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(set(range(total_samples)) - set(test_indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_targets = df_metadata.iloc[train_indices][\"target\"].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train set size: {len(train_indices)}')\n",
    "print(f'Test set size: {len(test_indices)}')"
   ]
  },
  {
   "source": [
    "## Extend dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_SURROGATES = 5                    # 5\n",
    "SURROGATE_METHOD = 'iaaft'          # 'iaaft', 'aaft'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'{PICKLE_FOLDER}/timeseries-{SURROGATE_METHOD}-{N_SURROGATES}.pickle', 'rb') as f:\n",
    "    ts_surrogates = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Extra data: {ts_surrogates.shape}')\n",
    "print(f'{ts_surrogates.shape[0] / total_samples} surrogates per sample')"
   ]
  },
  {
   "source": [
    "Extend only training data. Test set will consist of true data only."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train_surrogates = np.concatenate([ts_surrogates[N_SURROGATES*i:N_SURROGATES*i+N_SURROGATES] for i in train_indices], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_train_surrogates.shape"
   ]
  },
  {
   "source": [
    "## Prepare data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "source": [
    "### `Data` object fields\n",
    "\n",
    "- `data.x`: Node feature matrix with shape `[num_nodes, num_node_features]`\n",
    "\n",
    "- `data.edge_index`: Graph connectivity in COO format with shape `[2, num_edges]` and type `torch.long`\n",
    "\n",
    "- `data.edge_attr`: Edge feature matrix with shape `[num_edges, num_edge_features]`\n",
    "\n",
    "- `data.y`: Target to train against (may have arbitrary shape), e.g., node-level targets of shape `[num_nodes, *]` or graph-level targets of shape `[1, *]`\n",
    "\n",
    "- `data.pos`: Node position matrix with shape `[num_nodes, num_dimensions]`"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Select node features\n",
    "\n",
    "- onehot\n",
    "- timeseries\n",
    "- correlations"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_length_start = 50\n",
    "ts_length_end = ts_length\n",
    "\n",
    "# A part of signal serves as node features.\n",
    "def timeseries_in_nodes(i):\n",
    "    return torch.from_numpy(ts[i][:,ts_length_start:ts_length_end]).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each nodes contains its row from FC matrix.\n",
    "def correlations_in_nodes(i):\n",
    "    return torch.from_numpy(fc_matrix[i]).to(torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_and_ts_in_nodes(i):\n",
    "    corr_i = torch.from_numpy(fc_matrix[i]).to(torch.float32)\n",
    "    ts_i = torch.from_numpy(ts[i][:,ts_length_start:ts_length_end]).to(torch.float32)\n",
    "    return torch.cat([corr_i, ts_i], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each brain region is onehot encoded. See GIN for phenotype paper.\n",
    "def onehot_in_nodes(i):\n",
    "    return torch.diag(torch.ones(total_brain_regions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_in_nodes = correlations_in_nodes\n",
    "# total_brain_regions / ts_length_end-ts_length_start\n",
    "num_features_in_nodes = total_brain_regions"
   ]
  },
  {
   "source": [
    "### Create dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [Data(\n",
    "    x=features_in_nodes(i),  \n",
    "    edge_index=torch.from_numpy(np.asarray(np.nonzero(edge_index_matrix[i]))).to(torch.int64),\n",
    "    edge_attr=torch.from_numpy(fc_matrix[i][fc_matrix[i] != 0]).reshape((-1,1)).to(torch.float32),\n",
    "    y=torch.tensor([target], dtype=torch.int64)\n",
    ").to(device) for target, i in zip(train_targets, train_indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'True train data: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension.\n",
    "dataset_ext = [Data(\n",
    "    x=features_in_nodes(i),  \n",
    "    edge_index=torch.from_numpy(np.asarray(np.nonzero(edge_index_matrix[i]))).to(torch.int64),\n",
    "    edge_attr=torch.from_numpy(fc_matrix[i][fc_matrix[i] != 0]).repeat_interleave(num_features_in_nodes).reshape(-1,num_features_in_nodes).to(torch.float32),\n",
    "    y=torch.tensor([target], dtype=torch.int64)\n",
    ").to(device) for target, i, ts_surr in zip(\n",
    "    np.repeat(train_targets, N_SURROGATES),\n",
    "    np.repeat(train_indices, N_SURROGATES),\n",
    "    ts_train_surrogates\n",
    ")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Surrogate train data: {len(dataset_ext)}')"
   ]
  },
  {
   "source": [
    "### Join surrogates with true data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.extend(dataset_ext)\n",
    "\n",
    "# train_targets_ext = np.concatenate([train_targets, np.repeat(train_targets, N_SURROGATES)])\n",
    "# train_targets_ext.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Extended train data: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Data object')\n",
    "print(f'Edge index: {dataset[0].edge_index.shape}')\n",
    "print(f'Edge features: {dataset[0].edge_attr.shape}')\n",
    "print(f'Node features: {dataset[0].x.shape}')\n",
    "print(f'Target: {dataset[0].y.shape}')"
   ]
  },
  {
   "source": [
    "## Define model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union, Tuple\n",
    "from torch_geometric.typing import PairTensor, Adj, OptTensor, Size\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear, BatchNorm1d\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "\n",
    "class CGConvFixed(MessagePassing):\n",
    "\n",
    "    def __init__(self, channels: Union[int, Tuple[int, int]], dim: int = 0,\n",
    "                 aggr: str = 'add', batch_norm: bool = False,\n",
    "                 bias: bool = True, **kwargs):\n",
    "        super(CGConvFixed, self).__init__(aggr=aggr, **kwargs)\n",
    "        self.channels = channels\n",
    "        self.dim = dim\n",
    "        self.batch_norm = batch_norm\n",
    "\n",
    "        if isinstance(channels, int):\n",
    "            channels = (channels, channels)\n",
    "\n",
    "        self.lin_f = Linear(2*channels[0] + dim, channels[1], bias=bias)\n",
    "        self.lin_s = Linear(2*channels[0] + dim, channels[1], bias=bias)\n",
    "        self.bn = BatchNorm1d(channels[1])\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin_f.reset_parameters()\n",
    "        self.lin_s.reset_parameters()\n",
    "        self.bn.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, x: Union[Tensor, PairTensor], edge_index: Adj,\n",
    "                edge_attr: OptTensor = None, size: Size = None) -> Tensor:\n",
    "        if isinstance(x, Tensor):\n",
    "            x: PairTensor = (x, x)\n",
    "\n",
    "        # propagate_type: (x: PairTensor, edge_attr: OptTensor)\n",
    "        out = self.propagate(edge_index, x=x, edge_attr=edge_attr, size=size)\n",
    "        out = self.bn(out) if self.batch_norm else out\n",
    "        #out += x[1]\n",
    "        return out\n",
    "\n",
    "\n",
    "    def message(self, x_i, x_j, edge_attr: OptTensor) -> Tensor:\n",
    "        if edge_attr is None:\n",
    "            z = torch.cat([x_i, x_j], dim=-1)\n",
    "        else:\n",
    "            z = torch.cat([x_i, x_j, edge_attr], dim=-1)\n",
    "        return self.lin_f(z).sigmoid() * F.softplus(self.lin_s(z))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '{}({}, dim={})'.format(self.__class__.__name__, self.channels,\n",
    "                                       self.dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CGNet(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, hidden_channels, eps=0., p_dropout=0.5):\n",
    "        super(CGNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = CGConv((num_features_in_nodes, hidden_channels), dim=0)\n",
    "        self.conv2 = CGConv((hidden_channels, hidden_channels), dim=0)\n",
    "        self.conv3 = CGConv((hidden_channels, hidden_channels), dim=0)\n",
    "\n",
    "        self.fc1 = Linear(hidden_channels*3, 2)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, edge_attr, batch = data.x, data.edge_index, data.edge_attr, data.batch\n",
    "\n",
    "        # 1. Obtain node embeddings\n",
    "        # 2. Remember layer outputs.\n",
    "        x = self.conv1(x, edge_index)\n",
    "        out1 = global_mean_pool(x, batch)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        out2 = global_mean_pool(x, batch)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        out3 = global_mean_pool(x, batch)\n",
    "\n",
    "        # 3. Concatenate itermediate outputs.\n",
    "        x = torch.cat((out1, out2, out3), 1)\n",
    "\n",
    "        # 4. Fully connected for final classification.\n",
    "        x = self.fc1(x)     # CELoss already incorporates `log_softmax`.\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture.\n",
    "CGNet(hidden_channels=8)"
   ]
  },
  {
   "source": [
    "## Evaluation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_metrics(predicted, labels):\n",
    "    pred_positives = predicted == 1\n",
    "    label_positives = labels == 1\n",
    "\n",
    "    tp = (pred_positives & label_positives).sum().item()\n",
    "    tn = (~pred_positives & ~label_positives).sum().item()\n",
    "    fp = (pred_positives & ~label_positives).sum().item()\n",
    "    fn = (~pred_positives & label_positives).sum().item()\n",
    "\n",
    "    return tp, tn, fp, fn"
   ]
  },
  {
   "source": [
    "## Train model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FOLDS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings.\n",
    "EPOCHS = 500\n",
    "LR = 0.001\n",
    "MOMENTUM = 0.8\n",
    "OPTIMIZER = 'sgd'\n",
    "LOSS = 'ce'\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "VALIDATE_FREQ = 10\n",
    "\n",
    "HIDDEN_CHANNELS = 90\n",
    "\n",
    "STEP_SIZE = 50\n",
    "GAMMA = 0.5\n",
    "\n",
    "WEIGHT_DECAY = 0.0001\n",
    "\n",
    "settings_str = f'bs={BATCH_SIZE},e={EPOCHS},lr={LR},mom={MOMENTUM},opt={OPTIMIZER},loss={LOSS},step={STEP_SIZE},gamma={GAMMA},wd={WEIGHT_DECAY}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment folder.\n",
    "EXP_FOLDER = 'runs/cg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment.\n",
    "EXP_ID = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for kfold, (train_index, val_index) in enumerate(skf.split(np.zeros(len(train_targets)), train_targets)):\n",
    "\n",
    "    # Init TB writer.\n",
    "    experiment_str = f'id={EXP_ID:03d},fold={kfold},{settings_str}'\n",
    "    writer = SummaryWriter(f\"../{EXP_FOLDER}/{experiment_str}\")\n",
    "\n",
    "    # Init model.\n",
    "    net = CGNet(hidden_channels=HIDDEN_CHANNELS).to(device)\n",
    "    # optimizr = torch.optim.Adam(net.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    optimizr = torch.optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM, weight_decay=WEIGHT_DECAY)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    #scheduler = torch.optim.lr_scheduler.StepLR(optimizr, step_size=STEP_SIZE, gamma=GAMMA)\n",
    "    #scheduler = torch.optim.lr_scheduler.LambdaLR(optimizr, lr_lambda)\n",
    "\n",
    "    # Save architecture.\n",
    "    with open(f\"../{EXP_FOLDER}/{experiment_str}/architecture\", 'w', encoding=\"utf-8\") as f:\n",
    "        f.write(fc_folder + '\\n')\n",
    "        f.write(fc_file_binary + '\\n')\n",
    "        f.write(fc_file_real + '\\n')\n",
    "        f.write(features_in_nodes.__str__() + '\\n')\n",
    "        f.write('\\n'.join(experiment_str.split(',')) + '\\n\\n')\n",
    "        f.write(net.__str__() + '\\n\\n')\n",
    "        f.write(str(summary(net)))\n",
    "\n",
    "    # Prepare data.\n",
    "    X_train = [dataset[i] for i in train_index]\n",
    "    X_val = [dataset[i] for i in val_index]\n",
    "    \n",
    "    trainloader = DataLoader(X_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    valloader = DataLoader(X_val, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # Train.\n",
    "    for epoch in range(EPOCHS):\n",
    "        running_loss = 0.\n",
    "        \n",
    "        net.train()\n",
    "        for data in trainloader:\n",
    "            \n",
    "            optimizr.zero_grad()\n",
    "            outputs = net(data)   \n",
    "            loss = criterion(outputs, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizr.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        #scheduler.step()    # Update LR.\n",
    "        epoch_loss = running_loss / len(trainloader)\n",
    "        writer.add_scalar('training loss', epoch_loss, epoch)\n",
    "\n",
    "        running_loss = 0.\n",
    "\n",
    "        # Evaluate epoch.\n",
    "        tp, tn, fp, fn = 0, 0, 0, 0\n",
    "        total = 0\n",
    "\n",
    "        net.eval()\n",
    "        for data in valloader:\n",
    "\n",
    "            optimizr.zero_grad()\n",
    "            outputs = net(data) \n",
    "            loss = criterion(outputs, data.y)  # Compute the loss.\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if (epoch+1) % VALIDATE_FREQ == 0:\n",
    "                predicted = outputs.argmax(dim=1)\n",
    "                labels = data.y.view(-1)\n",
    "\n",
    "                # Update.\n",
    "                tp_, tn_, fp_, fn_ = evaluation_metrics(predicted, labels)\n",
    "                tp += tp_; tn += tn_; fp += fp_; fn += fn_\n",
    "                total += data.y.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(valloader)\n",
    "        writer.add_scalar('validation loss', epoch_loss, epoch)\n",
    "        \n",
    "        if (epoch+1) % VALIDATE_FREQ == 0:\n",
    "            writer.add_scalar('validation accuracy', (tp + tn) / total, epoch)\n",
    "            writer.add_scalar('validation precision', tp / (tp + fp) if (tp + fp) > 0 else 0, epoch)\n",
    "            writer.add_scalar('validation recall', tp / (tp + fn), epoch)\n",
    "    \n",
    "    # Single fold during exploration.\n",
    "    #break\n",
    "\n",
    "print('Finished training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}