{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.6 64-bit ('.venv': venv)"
  },
  "interpreter": {
   "hash": "9c40931cbb4a31228df7bc840fc8745c5509fc7c9b0d60dae36a005e5fe0dba9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GIN < FC"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "import pickle\r\n",
    "import pandas as pd \r\n",
    "import matplotlib.pyplot as plt\r\n",
    "from enum import Enum, auto\r\n",
    "import typing\r\n",
    "from typing import List, Union\r\n",
    "import copy\r\n",
    "\r\n",
    "import torch\r\n",
    "import torch.nn.functional as F\r\n",
    "import torch.nn as nn\r\n",
    "from torch.utils.tensorboard import SummaryWriter\r\n",
    "\r\n",
    "from torch_geometric.nn import GINConv, global_add_pool\r\n",
    "from torch_geometric.data import Data, DataLoader\r\n",
    "\r\n",
    "from torchinfo import summary\r\n",
    "from sklearn.model_selection import StratifiedKFold"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DATA_FOLDER = '../data'\r\n",
    "PICKLE_FOLDER = '../pickles'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_metadata = pd.read_csv(f'{DATA_FOLDER}/patients-cleaned.csv', index_col=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df_metadata.head(2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select connectivity dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "THRESHOLD = 0.1                                          # 0.01, 0.05, 0.1, 0.15\r\n",
    "N = 20                                                   # 3, 5, 7, 10, 15, 20, 40\r\n",
    "CORR_TYPE = 'pearson'                                    # 'pearson', 'spearman', 'partial-pearson'\r\n",
    "THRESHOLD_METHOD = 'abs-group-avg-diff'                  # 'abs-sample-diff', 'abs-group-avg-diff'\r\n",
    "THRESHOLD_TYPE = 'max'                                   # 'min', 'max' or for kNN 'small', 'large'\r\n",
    "KNN = False                                              # Whether all or only top N neigbors are taken"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fc_folder = f'{PICKLE_FOLDER}/fc-{CORR_TYPE}{\"-knn\" if KNN else \"\"}-{THRESHOLD_METHOD}'\r\n",
    "\r\n",
    "# Try Gini or SGD.\r\n",
    "# fc_folder = f'{PICKLE_FOLDER}/fc-{CORR_TYPE}-gini'\r\n",
    "# fc_folder = f'{PICKLE_FOLDER}/fc-{CORR_TYPE}-sgd'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fc_file_binary = f'{fc_folder}/{THRESHOLD_TYPE}-{f\"knn-{N}\" if KNN else f\"th-{THRESHOLD}\"}-binary.pickle'\r\n",
    "fc_file_real = f'{fc_folder}/{THRESHOLD_TYPE}-{f\"knn-{N}\" if KNN else f\"th-{THRESHOLD}\"}-real.pickle'\r\n",
    "\r\n",
    "# fc_file_binary = f'{fc_folder}/binary.pickle'\r\n",
    "# fc_file_real = f'{fc_folder}/real.pickle'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(fc_file_binary, 'rb') as f:\r\n",
    "    edge_index_matrix = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(fc_file_real, 'rb') as f:\r\n",
    "    fc_matrix = pickle.load(f)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "total_samples, total_brain_regions, _ = edge_index_matrix.shape\r\n",
    "edge_index_matrix.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fc_matrix.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(f'{PICKLE_FOLDER}/test-indices.pickle', 'rb') as f:\r\n",
    "    test_indices = pickle.load(f)\r\n",
    "    \r\n",
    "train_indices = list(set(range(total_samples)) - set(test_indices))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_targets = df_metadata.iloc[train_indices][\"target\"].reset_index(drop=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'Train set size: {len(train_indices)}')\r\n",
    "print(f'Test set size: {len(test_indices)}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
    "device"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### `Data` object fields\r\n",
    "\r\n",
    "- `data.x`: Node feature matrix with shape `[num_nodes, num_node_features]`\r\n",
    "\r\n",
    "- `data.edge_index`: Graph connectivity in COO format with shape `[2, num_edges]` and type `torch.long`\r\n",
    "\r\n",
    "- `data.edge_attr`: Edge feature matrix with shape `[num_edges, num_edge_features]`\r\n",
    "\r\n",
    "- `data.y`: Target to train against (may have arbitrary shape), e.g., node-level targets of shape `[num_nodes, *]` or graph-level targets of shape `[1, *]`\r\n",
    "\r\n",
    "- `data.pos`: Node position matrix with shape `[num_nodes, num_dimensions]`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Select node features\r\n",
    "\r\n",
    "- onehot\r\n",
    "- correlations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Each nodes contains its row from FC matrix.\r\n",
    "def correlations_in_nodes(i):\r\n",
    "    return torch.from_numpy(fc_matrix[i]).to(torch.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Each brain region is onehot encoded. See GIN for phenotype paper.\r\n",
    "def onehot_in_nodes(i):\r\n",
    "    return torch.diag(torch.ones(total_brain_regions))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "features_in_nodes = correlations_in_nodes\r\n",
    "num_features_in_nodes = total_brain_regions   "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataset"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset = [Data(\r\n",
    "    x=features_in_nodes(i),  \r\n",
    "    edge_index=torch.from_numpy(np.asarray(np.nonzero(edge_index_matrix[i]))).to(torch.int64),\r\n",
    "    # y=torch.tensor([[1, 0]]  if target == 0 else [[0, 1]], dtype=torch.int64)\r\n",
    "    y=torch.tensor([target], dtype=torch.int64)\r\n",
    ").to(device) for target, i in zip(train_targets, train_indices)]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(f'True train data: {len(dataset)}')\r\n",
    "\r\n",
    "print('Data object')\r\n",
    "print(f'Edge index: {dataset[0].edge_index.shape}')\r\n",
    "print(f'Node features: {dataset[0].x.shape}')\r\n",
    "print(f'Target: {dataset[0].y.shape}')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Define GIN < FC architectures"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConnectivityEmbedding(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Learns connectivity between nodes. For each node a weighted combination of all the nodes is learned.\r\n",
    "\r\n",
    "    Input: [batch_size, num_nodes, num_features]\r\n",
    "    Output: [batch_size, num_nodes, num_features]\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, size, dropout: 0.0):\r\n",
    "        super(ConnectivityEmbedding, self).__init__()\r\n",
    "        # Initialize with fully connected graph.\r\n",
    "        self.fc_matrix = nn.Parameter(torch.ones(size, size), requires_grad=True)\r\n",
    "        self.dropout = nn.Dropout(p=dropout)\r\n",
    "\r\n",
    "\r\n",
    "    def toggle_gradients(self, requires_grad):\r\n",
    "        self.fc_matrix.requires_grad = requires_grad\r\n",
    "\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # There is no non-linearity since we are just combining nodes.\r\n",
    "        return self.dropout(torch.matmul(self.fc_matrix, x))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConnectivityMLP(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Runs node features through MLP.\r\n",
    "\r\n",
    "    Input: [batch_size, num_nodes, num_in_features]\r\n",
    "    Output: [batch_size, num_nodes, num_out_features]\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, size_in, size_out, dropout):\r\n",
    "        super(ConnectivityMLP, self).__init__()\r\n",
    "        self.fc = nn.Linear(size_in, size_out)\r\n",
    "        self.dropout = nn.Dropout(p=dropout)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        return F.relu(self.dropout(self.fc(x)))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConnectivityMode(Enum):\r\n",
    "    \"\"\"\r\n",
    "    Determines how is connectivity matrix obtained.\r\n",
    "\r\n",
    "    FIXED: Use handmade FC matrix.\r\n",
    "    START: Learn FC matrix only on raw input features.\r\n",
    "    SINGLE: Learn FC matrix on raw input features as well as all subsequent feature mapping layers.\r\n",
    "    MULTIPLE: Learn new FC matrix before every feature mapping layer.\r\n",
    "    \"\"\"\r\n",
    "    FIXED = auto()\r\n",
    "    START = auto()\r\n",
    "    SINGLE = auto()\r\n",
    "    MULTIPLE = auto()\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "class ConnectivitySublayer(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Combines neighborhood connectivity with MLP transformation.\r\n",
    "\r\n",
    "    Input: [batch_size, num_nodes, num_in_features]\r\n",
    "    Output: [batch_size, num_nodes, num_out_features]\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, sublayer_id: int, size_in: int, size_out: int, dropout: float, mode: ConnectivityMode, **mode_kwargs):\r\n",
    "        super(ConnectivitySublayer, self).__init__()\r\n",
    "\r\n",
    "        self.id = sublayer_id\r\n",
    "        # Create new FC matrix for this sublayer.\r\n",
    "        if mode == ConnectivityMode.MULTIPLE:\r\n",
    "            self.fc_matrix = ConnectivityEmbedding(size_in, dropout=mode_kwargs['dropout'])\r\n",
    "        # Used passed in FC matrix.\r\n",
    "        else:\r\n",
    "            self.fc_matrix = mode_kwargs['fc_matrix']\r\n",
    "            # Switch of gradients for subsequent layers in start mode.\r\n",
    "            if mode == mode.START and sublayer_id > 0:\r\n",
    "                self.fc_matrix.toggle_gradients(requires_grad=False)\r\n",
    "\r\n",
    "        # Feature mapping layer.\r\n",
    "        self.mlp = ConnectivityMLP(size_in, size_out, dropout)\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # Aggregate feature vectors based on connectivity neighborhood.\r\n",
    "        x = self.fc_matrix(x)\r\n",
    "        # Map features.\r\n",
    "        x = self.mlp(x)\r\n",
    "        return x       \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "source": [
    "class ConnectivityDenseNet(nn.Module):\r\n",
    "    \"\"\"\r\n",
    "    Emulates Graph isomorphism network using a fully connected alternative.\r\n",
    "    \"\"\"\r\n",
    "    def __init__(\r\n",
    "        self, \r\n",
    "        num_nodes: int, \r\n",
    "        mode: ConnectivityMode, \r\n",
    "        num_in_features: int, \r\n",
    "        num_hidden_features: Union[int, List[int]],\r\n",
    "        dropout: float = 0.5,\r\n",
    "        connectivity_dropout: float = 0.0, \r\n",
    "        num_sublayers: int = -1,\r\n",
    "        readout: str = 'add', \r\n",
    "        **mode_kwargs\r\n",
    "    ):\r\n",
    "        super(ConnectivityDenseNet, self).__init__()\r\n",
    "\r\n",
    "        self.mode = mode\r\n",
    "        self.fc_matrix = None\r\n",
    "        # Set passed in FC matrix.\r\n",
    "        if mode == ConnectivityMode.FIXED:\r\n",
    "            self.fc_matrix = lambda x: torch.matmul(mode_kwargs['fc_matrix'], x)\r\n",
    "        # Create single FC matrix that will be learned only at the beggining.\r\n",
    "        # or\r\n",
    "        # Create single FC matrix that will be learned throughout.\r\n",
    "        elif (mode == ConnectivityMode.START) or (mode == ConnectivityMode.SINGLE):\r\n",
    "            self.fc_matrix = ConnectivityEmbedding(num_nodes, dropout=connectivity_dropout)\r\n",
    "        # Else `ConnectivityMode.MULTIPLE`, let each sublayer create its own FC matrix.\r\n",
    "        self.mode_kwargs = {\r\n",
    "            'fc_matrix': self.fc_matrix,\r\n",
    "            'dropout': connectivity_dropout\r\n",
    "        }\r\n",
    "\r\n",
    "        # Prepare feature mapping dimensions.\r\n",
    "        if type(num_hidden_features) is int:\r\n",
    "            num_out_features = np.repeat(num_hidden_features, num_sublayers)\r\n",
    "        num_in_features = copy.copy(num_out_features)\r\n",
    "        num_out_features[0] = num_nodes\r\n",
    "\r\n",
    "        # Create model stacked from sublayers: connectivity + feature mapping.\r\n",
    "        self.sublayers = nn.ModuleList([\r\n",
    "            ConnectivitySublayer(\r\n",
    "                i, size_in, size_out, dropout=dropout, mode=mode, mode_kwargs=self.mode_kwargs\r\n",
    "            ) for i, (size_in, size_out) in enumerate(zip(num_in_features, num_out_features))\r\n",
    "        ])\r\n",
    "\r\n",
    "        # Classification head.\r\n",
    "        self.readout = readout\r\n",
    "        self.fc = nn.Linear(num_out_features[-1], 2)\r\n",
    "        \r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        # Run sample through model.\r\n",
    "        for sublayer in self.sublayers:\r\n",
    "            x = sublayer(x)\r\n",
    "\r\n",
    "        # Turn on gradients for start mode back on.\r\n",
    "        if self.mode == ConnectivityMode.START:\r\n",
    "            self.fc_matrix.toggle_gradients(requires_grad=True)\r\n",
    "\r\n",
    "        # Binary classification head.\r\n",
    "        # Readout across nodes.\r\n",
    "        if self.readout == 'add':\r\n",
    "            x = torch.sum(x, dim=1)\r\n",
    "        elif self.readout == 'mean':\r\n",
    "            x = torch.mean(x, dim=1)\r\n",
    "        if self.readout == 'max':\r\n",
    "            x = torch.max(x, dim=1)\r\n",
    "\r\n",
    "        # Return binary logits.\r\n",
    "        return self.fc(x)\r\n",
    "        \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class MLP(torch.nn.Module):\r\n",
    "    def __init__(self, inchan, outchan):\r\n",
    "        super(MLP, self).__init__()\r\n",
    "        \r\n",
    "        self.fc = Linear(inchan, outchan)\r\n",
    "        self.activation = ReLU()\r\n",
    "        \r\n",
    "    def forward(self, x):\r\n",
    "        x = self.activation(self.fc(x))\r\n",
    "        \r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class GIN(torch.nn.Module):\r\n",
    "    \r\n",
    "    def __init__(self, depth, hidchan=total_brain_regions, eps=0.):\r\n",
    "        super(GIN, self).__init__()\r\n",
    "\r\n",
    "        self.convs = torch.nn.ModuleList([GINConv(MLP(hidchan, hidchan), eps=eps) for _ in range(depth)])\r\n",
    "        self.final_fc = Linear(hidchan, 2)\r\n",
    "\r\n",
    "    def forward(self, data):\r\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\r\n",
    "\r\n",
    "        # Message passing.\r\n",
    "        for conv in self.convs:\r\n",
    "            x = conv(x, edge_index)\r\n",
    "\r\n",
    "        # Readout.\r\n",
    "        x = global_add_pool(x, batch)\r\n",
    "\r\n",
    "        # Final FC.\r\n",
    "        x = self.final_fc(x)\r\n",
    "        \r\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Architecture FC.\r\n",
    "# summary(FC(depth=3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Architecture GIN.\r\n",
    "summary(GIN(depth=3))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def evaluation_metrics(predicted, labels):\r\n",
    "    pred_positives = predicted == 1\r\n",
    "    label_positives = labels == 1\r\n",
    "\r\n",
    "    tp = (pred_positives & label_positives).sum().item()\r\n",
    "    tn = (~pred_positives & ~label_positives).sum().item()\r\n",
    "    fp = (pred_positives & ~label_positives).sum().item()\r\n",
    "    fn = (~pred_positives & label_positives).sum().item()\r\n",
    "\r\n",
    "    return tp, tn, fp, fn"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NUM_FOLDS = 3"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "skf = StratifiedKFold(n_splits=NUM_FOLDS, random_state=42, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Settings.\r\n",
    "EPOCHS = 200\r\n",
    "LR = 0.001\r\n",
    "MOMENTUM = 0.5\r\n",
    "OPTIMIZER = 'adam'\r\n",
    "LOSS = 'ce'\r\n",
    "BATCH_SIZE = 2\r\n",
    "\r\n",
    "VALIDATE_FREQ = 10\r\n",
    "\r\n",
    "DEPTH = 3\r\n",
    "EPS = 0.2\r\n",
    "\r\n",
    "STEP_SIZE = 50\r\n",
    "GAMMA = 0.5\r\n",
    "\r\n",
    "WEIGHT_DECAY = 0.0001\r\n",
    "\r\n",
    "settings_str = f'bs={BATCH_SIZE},e={EPOCHS},lr={LR},mom={MOMENTUM},opt={OPTIMIZER},loss={LOSS},step={STEP_SIZE},gamma={GAMMA},wd={WEIGHT_DECAY},eps={EPS}'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Experiment folder.\r\n",
    "EXP_FOLDER = 'runs/fc-vs-gin'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Experiment.\r\n",
    "EXP_ID = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for kfold, (train_index, val_index) in enumerate(skf.split(np.zeros(len(train_targets)), train_targets)):\r\n",
    "\r\n",
    "    # Init TB writer.\r\n",
    "    experiment_str = f'id={EXP_ID:03d},fold={kfold},{settings_str}'\r\n",
    "    writer_FC = SummaryWriter(f\"../{EXP_FOLDER}/FC/{experiment_str}\")\r\n",
    "    writer_GIN = SummaryWriter(f\"../{EXP_FOLDER}/GIN/{experiment_str}\")\r\n",
    "\r\n",
    "    # Init models.\r\n",
    "    net_GIN = GIN(depth=DEPTH, eps=EPS).to(device)\r\n",
    "    optimizr_GIN = torch.optim.Adam(net_GIN.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\r\n",
    "    criterion_GIN = torch.nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "    net_FC = FC(depth=DEPTH).to(device)\r\n",
    "    optimizr_FC = torch.optim.Adam(net_FC.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\r\n",
    "    criterion_FC = torch.nn.CrossEntropyLoss()\r\n",
    "\r\n",
    "    # Save architecture.\r\n",
    "    with open(f\"../{EXP_FOLDER}/GIN/{experiment_str}/architecture\", 'w', encoding=\"utf-8\") as f:\r\n",
    "        f.write(fc_folder + '\\n')\r\n",
    "        f.write(fc_file_binary + '\\n')\r\n",
    "        f.write(fc_file_real + '\\n')\r\n",
    "        f.write(features_in_nodes.__str__() + '\\n')\r\n",
    "        f.write('\\n'.join(experiment_str.split(',')) + '\\n\\n')\r\n",
    "        f.write(net_FC.__str__() + '\\n\\n')\r\n",
    "        f.write(net_GIN.__str__() + '\\n\\n')\r\n",
    "        f.write(str(summary(net_FC)))\r\n",
    "        f.write(str(summary(net_GIN)))\r\n",
    "\r\n",
    "    # Prepare data.\r\n",
    "    X_train = [dataset[i] for i in train_index]\r\n",
    "    X_val = [dataset[i] for i in val_index]\r\n",
    "    \r\n",
    "    trainloader = DataLoader(X_train, batch_size=BATCH_SIZE, shuffle=True)\r\n",
    "    valloader = DataLoader(X_val, batch_size=BATCH_SIZE, shuffle=False)\r\n",
    "\r\n",
    "    # Train.\r\n",
    "    for epoch in range(EPOCHS):\r\n",
    "        running_loss_GIN = 0.\r\n",
    "        running_loss_FC = 0.\r\n",
    "        \r\n",
    "        net_FC.train()\r\n",
    "        net_GIN.train()\r\n",
    "\r\n",
    "        for data in trainloader:\r\n",
    "            \r\n",
    "            optimizr_GIN.zero_grad()\r\n",
    "            optimizr_FC.zero_grad()\r\n",
    "\r\n",
    "            outputs_GIN = net_GIN(data)\r\n",
    "            outputs_FC = net_FC(data)\r\n",
    "\r\n",
    "            loss_GIN = criterion_GIN(outputs_GIN, data.y)\r\n",
    "            loss_FC = criterion_FC(outputs_FC, data.y)\r\n",
    "\r\n",
    "            loss_GIN.backward()\r\n",
    "            loss_FC.backward()\r\n",
    "\r\n",
    "            optimizr_GIN.step()\r\n",
    "            optimizr_FC.step()\r\n",
    "\r\n",
    "            running_loss_GIN += loss_GIN.item()\r\n",
    "            running_loss_FC += loss_FC.item()\r\n",
    "\r\n",
    "        epoch_loss_GIN = running_loss_GIN / len(trainloader)\r\n",
    "        epoch_loss_FC = running_loss_FC / len(trainloader)\r\n",
    "\r\n",
    "        writer_GIN.add_scalar('training loss', epoch_loss_GIN, epoch)\r\n",
    "        writer_FC.add_scalar('training loss', epoch_loss_FC, epoch)\r\n",
    "\r\n",
    "        running_loss_GIN = 0.\r\n",
    "        running_loss_FC = 0.\r\n",
    "\r\n",
    "        # Evaluate epoch.\r\n",
    "        tp_GIN, tn_GIN, fp_GIN, fn_GIN = 0, 0, 0, 0\r\n",
    "        tp_FC, tn_FC, fp_FC, fn_FC = 0, 0, 0, 0\r\n",
    "        total = 0\r\n",
    "\r\n",
    "        net_GIN.eval()\r\n",
    "        net_FC.eval()\r\n",
    "        for data in valloader:\r\n",
    "            optimizr_GIN.zero_grad()\r\n",
    "            optimizr_FC.zero_grad()\r\n",
    "\r\n",
    "            outputs_GIN = net_GIN(data)\r\n",
    "            outputs_FC = net_GIN(data)\r\n",
    "\r\n",
    "            loss_GIN = criterion_GIN(outputs_GIN, data.y)\r\n",
    "            loss_FC = criterion_FC(outputs_FC, data.y)\r\n",
    "\r\n",
    "            running_loss_GIN += loss_GIN.item()\r\n",
    "            running_loss_FC += loss_FC.item()\r\n",
    "\r\n",
    "            if (epoch+1) % VALIDATE_FREQ == 0:\r\n",
    "                predicted_GIN = outputs_GIN.argmax(dim=1)\r\n",
    "                predicted_FC = outputs_FC.argmax(dim=1)\r\n",
    "\r\n",
    "                # labels = torch.nonzero(data.y, as_tuple=True)[1]\r\n",
    "                labels = data.y.view(-1)\r\n",
    "\r\n",
    "                # Update.\r\n",
    "                _tp_GIN, _tn_GIN, _fp_GIN, _fn_GIN = evaluation_metrics(predicted_GIN, labels)\r\n",
    "                _tp_FC, _tn_FC, _fp_FC, _fn_FC = evaluation_metrics(predicted_FC, labels)\r\n",
    "\r\n",
    "                tp_GIN += _tp_GIN; tn_GIN += _tn_GIN; fp_GIN += _fp_GIN; fn_GIN += _fn_GIN\r\n",
    "                tp_FC += _tp_FC; tn_FC += _tn_FC; fp_FC += _fp_FC; fn_FC += _fn_FC\r\n",
    "\r\n",
    "                total += data.y.size(0)\r\n",
    "\r\n",
    "        epoch_loss_GIN = running_loss_GIN / len(valloader)\r\n",
    "        epoch_loss_FC = running_loss_FC / len(valloader)\r\n",
    "\r\n",
    "        writer_GIN.add_scalar('validation loss', epoch_loss_GIN, epoch)\r\n",
    "        writer_FC.add_scalar('validation loss', epoch_loss_FC, epoch)\r\n",
    "\r\n",
    "        if (epoch+1) % VALIDATE_FREQ == 0:\r\n",
    "            writer_GIN.add_scalar('validation accuracy', (tp_GIN + tn_GIN) / total, epoch)\r\n",
    "            writer_GIN.add_scalar('validation precision', tp_GIN / (tp_GIN + fp_GIN) if (tp_GIN + fp_GIN) > 0 else 0, epoch)\r\n",
    "            writer_GIN.add_scalar('validation recall', tp_GIN / (tp_GIN + fn_GIN), epoch)\r\n",
    "    \r\n",
    "            writer_FC.add_scalar('validation accuracy', (tp_FC + tn_FC) / total, epoch)\r\n",
    "            writer_FC.add_scalar('validation precision', tp_FC / (tp_FC + fp_FC) if (tp_FC + fp_FC) > 0 else 0, epoch)\r\n",
    "            writer_FC.add_scalar('validation recall', tp_FC / (tp_FC + fn_FC), epoch)\r\n",
    "\r\n",
    "    # Single fold during exploration.\r\n",
    "    #break\r\n",
    "\r\n",
    "print('Finished training')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ]
}